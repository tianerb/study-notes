{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Notes for* [colah's blog : Visual Information Theory (2015-09)](http://colah.github.io/posts/2015-09-Visual-Information/)\n",
    "### Visualizing Probability Distributions\n",
    "### Aside: Simpson's Paradox\n",
    "One thing to keep in mind when designing an experiment/ABtest.\n",
    "[vector interpretation](https://en.wikipedia.org/wiki/Simpson%27s_paradox#Vector_interpretation)\n",
    "### Codes\n",
    "### Variable-Length Codes\n",
    "Entropy = Optimal Average Code Length = $\\sum_xp(x)*\\text{codeLength}(x)$\n",
    "### The Space of Codewords\n",
    "**prefix property / prefix codes**\n",
    "Suppose we already decoded previous codes, then as long as every code word is not prefix of other code word, we can just decode the first code word starting from the current digit, then continue. This way, the codes can be uniquely decoded.\n",
    "\n",
    "<img src=\"http://colah.github.io/posts/2015-09-Visual-Information/img/CodeSpaceUsed.png\"  width=\"300px\"/>\n",
    "\n",
    "### Optimal Encodings\n",
    "$e^{-x} = \\int_x^{\\infty} e^{-t}dt$\n",
    "\n",
    "\n",
    "<img src=\"http://colah.github.io/posts/2015-09-Visual-Information/img/code-cost.png\"  width=\"400px\"/>\n",
    "\n",
    "Average Length Contribution = p(x) * codeLength(x)\n",
    "Codeword Cost = 2^{-codeLength}\n",
    "Optimal Encoding: \n",
    "> Distribute our budget in proportion to how common an event is. So, if one event happens 50% of the time, we spend 50% of our budget buying a short codeword for it. But if an event only happens 1% of the time, we only spend 1% of our budget, because we don’t care very much if the codeword is long\n",
    "\n",
    "<img src=\"http://colah.github.io/posts/2015-09-Visual-Information/img/code-auction-balanced-noderivs.png\"  width=\"400px\"/>\n",
    "\n",
    "### Entropy\n",
    "$\\frac{1}{2^{L(x)}} = \\text{cost}(x)$\n",
    "optimal encoding: $\\text{cost}(x) = p(x)$\n",
    "$L(x) = \\log_2\\frac{1}{\\text{cost}} = \\log_2\\frac{1}{p(x)}$\n",
    " $H(p) = \\sum_xp(x)\\log_2\\frac{1}{p(x)}=-\\sum_xp(x)\\log_2p(x)$\n",
    "### Cross-Entropy\n",
    "> This length – the average length of communicating an event from one distribution with the optimal code for another distribution – is called the cross-entropy.\n",
    "\n",
    "Communicating $q$ using optimal code for $p$\n",
    "$H_p(q)=\\sum_xq(x)\\log_2\\frac{1}{p(x)}$\n",
    "\n",
    "The **Kullback–Leibler divergence** (KL divergence) of p with respect to q:\n",
    "$D_q(p) = H_q(p) - H(p)=\\sum_xp(x)\\log_2\\frac{p(x)}{q(x)} = \\sum_xp(x)(\\log_2p(x) - \\log_2q(x))$\n",
    "A measure of difference between two probability distributions.\n",
    "\n",
    "Cross entropy and KL divergence are asymmetric.\n",
    "### Entropy and Multiple Variables\n",
    "**Joint Entropy**\n",
    "$H(X,Y) = \\sum_{x,y}p(x,y)\\log_2\\frac{1}{p(x,y)}$\n",
    "**Conditional Entropy**\n",
    "$H(X|Y) = \\sum_yp(y)\\sum_xp(x|y)\\log_2\\frac{1}{p(x|y)}=\\sum_{x,y}p(x,y)\\log_2\\frac{1}{p(x|y)}$\n",
    "\n",
    "### Mutual Information\n",
    "- $H(X,Y)\\geq H(X)\\geq H(X|Y)$\n",
    "\n",
    "- info of X and Y = info of Y + extra info from X which Y doesn't tell you\n",
    "$H(X,Y) = H(Y)+H(X|Y)$\n",
    "\n",
    "- **Mutual Information**\n",
    "$I(X,Y) = H(X)+H(Y)-H(X,Y)\n",
    "=\\sum_{x,y}p(x,y)\\log_2\\frac{p(x,y)}{p(x)p(y)}$\n",
    ">It’s the KL divergence of P(X,Y) and its naive approximation P(X)P(Y). That is, it’s the number of bits you save representing X and Y if you understand the relationship between them instead of assuming they’re independent.\n",
    "\n",
    "- **Variation of Information**\n",
    "$V(X,Y) = H(X,Y)-I(X,Y)$\n",
    "\n",
    "<img src=\"http://colah.github.io/posts/2015-09-Visual-Information/img/Hxy-info.png\"  width=\"300px\"/>\n",
    "\n",
    "### Fractional Bits\n",
    "Optimal length $L(x) =  \\log_2\\frac{1}{p(x)}$ may not be integer.\n",
    "Instead of trying to encode `a` and `b`,\n",
    "we can try to encode all codes of\n",
    "length 2:\n",
    "aa,ab,ba,bb;\n",
    "length 3:\n",
    "aaa, aab, aba, abb, baa, bab, bba, bbb\n",
    "...\n",
    "Then the average length spent on `a` and `b` would be closer and closer to the optimal length.\n",
    ">As n tends to infinity, the overhead due to rounding our code would vanish, and the number of bits per codeword would approach the entropy.\n",
    "\n",
    "We can also use more complicated tricks to approach the entropy limit, or use a different encoding scheme: [Arithmetic coding](https://en.wikipedia.org/wiki/Arithmetic_coding) \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
